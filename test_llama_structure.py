#!/usr/bin/env python3
"""
æµ‹è¯•Llamaæ¨¡å‹ç»“æ„æ£€æµ‹
"""

import torch
import torch.nn as nn
import sys
import os

# æ·»åŠ å½“å‰ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

try:
    import wrapper_svd as wrapper
    print("âœ“ æˆåŠŸå¯¼å…¥ wrapper_svd")
except ImportError as e:
    print(f"âœ— å¯¼å…¥ wrapper_svd å¤±è´¥: {e}")
    sys.exit(1)

def test_llama_structure():
    """æµ‹è¯•Llamaæ¨¡å‹ç»“æ„æ£€æµ‹"""
    print("\n=== æµ‹è¯•Llamaæ¨¡å‹ç»“æ„æ£€æµ‹ ===")
    
    # åˆ›å»ºæ¨¡æ‹Ÿçš„Llamaæ¨¡å‹ç»“æ„
    class MockLlamaModel(nn.Module):
        def __init__(self):
            super().__init__()
            # æ¨¡æ‹ŸLlamaçš„ç»“æ„: model.model.layers
            self.model = type('obj', (object,), {
                'layers': nn.ModuleList([
                    type('obj', (object,), {
                        'self_attn': type('obj', (object,), {}),
                        'mlp': type('obj', (object,), {})
                    }) for _ in range(32)
                ])
            })
        
        def forward(self, x):
            return x
    
    try:
        model = MockLlamaModel()
        tokenizer = None
        config = {}
        device = torch.device('cpu')
        
        print("åˆ›å»ºæ¨¡æ‹ŸLlamaæ¨¡å‹...")
        wrapper_instance = wrapper.ModelWrapper(model, tokenizer, config, device)
        
        print(f"âœ“ æˆåŠŸæ£€æµ‹åˆ°å±‚æ•°: {wrapper_instance.num_layers}")
        
        # æµ‹è¯•å±æ€§è·¯å¾„ç”Ÿæˆ
        print("\næµ‹è¯•å±æ€§è·¯å¾„ç”Ÿæˆ...")
        attn_path = wrapper_instance._get_arribute_path(0, 'attn')
        mlp_path = wrapper_instance._get_arribute_path(0, 'mlp')
        hidden_path = wrapper_instance._get_arribute_path(0, 'hidden')
        
        print(f"âœ“ attnè·¯å¾„: {attn_path}")
        print(f"âœ“ mlpè·¯å¾„: {mlp_path}")
        print(f"âœ“ hiddenè·¯å¾„: {hidden_path}")
        
        # æµ‹è¯•åµŒå¥—å±æ€§è·å–
        print("\næµ‹è¯•åµŒå¥—å±æ€§è·å–...")
        try:
            attn_module = wrapper_instance._get_nested_attr(attn_path)
            print(f"âœ“ æˆåŠŸè·å–attnæ¨¡å—: {type(attn_module)}")
        except Exception as e:
            print(f"âœ— è·å–attnæ¨¡å—å¤±è´¥: {e}")
        
        return True
        
    except Exception as e:
        print(f"âœ— æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_gpt_structure():
    """æµ‹è¯•GPTæ¨¡å‹ç»“æ„æ£€æµ‹"""
    print("\n=== æµ‹è¯•GPTæ¨¡å‹ç»“æ„æ£€æµ‹ ===")
    
    # åˆ›å»ºæ¨¡æ‹Ÿçš„GPTæ¨¡å‹ç»“æ„
    class MockGPTModel(nn.Module):
        def __init__(self):
            super().__init__()
            # æ¨¡æ‹ŸGPTçš„ç»“æ„: transformer.h
            self.transformer = type('obj', (object,), {
                'h': nn.ModuleList([
                    type('obj', (object,), {
                        'attn': type('obj', (object,), {}),
                        'mlp': type('obj', (object,), {})
                    }) for _ in range(12)
                ])
            })
        
        def forward(self, x):
            return x
    
    try:
        model = MockGPTModel()
        tokenizer = None
        config = {}
        device = torch.device('cpu')
        
        print("åˆ›å»ºæ¨¡æ‹ŸGPTæ¨¡å‹...")
        wrapper_instance = wrapper.ModelWrapper(model, tokenizer, config, device)
        
        print(f"âœ“ æˆåŠŸæ£€æµ‹åˆ°å±‚æ•°: {wrapper_instance.num_layers}")
        
        # æµ‹è¯•å±æ€§è·¯å¾„ç”Ÿæˆ
        print("\næµ‹è¯•å±æ€§è·¯å¾„ç”Ÿæˆ...")
        attn_path = wrapper_instance._get_arribute_path(0, 'attn')
        mlp_path = wrapper_instance._get_arribute_path(0, 'mlp')
        hidden_path = wrapper_instance._get_arribute_path(0, 'hidden')
        
        print(f"âœ“ attnè·¯å¾„: {attn_path}")
        print(f"âœ“ mlpè·¯å¾„: {mlp_path}")
        print(f"âœ“ hiddenè·¯å¾„: {hidden_path}")
        
        return True
        
    except Exception as e:
        print(f"âœ— æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    print("å¼€å§‹æµ‹è¯•æ¨¡å‹ç»“æ„æ£€æµ‹...")
    
    tests = [
        test_llama_structure,
        test_gpt_structure
    ]
    
    passed = 0
    total = len(tests)
    
    for test in tests:
        try:
            if test():
                passed += 1
        except Exception as e:
            print(f"âœ— æµ‹è¯•å¼‚å¸¸: {e}")
    
    print(f"\n=== æµ‹è¯•ç»“æœ ===")
    print(f"é€šè¿‡: {passed}/{total}")
    
    if passed == total:
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼æ¨¡å‹ç»“æ„æ£€æµ‹å·¥ä½œæ­£å¸¸ã€‚")
    else:
        print("âŒ éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥ä»£ç ã€‚")
